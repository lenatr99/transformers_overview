
@article{rives_biological_2021,
	title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2016239118},
	doi = {10.1073/pnas.2016239118},
	abstract = {Significance
            Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.
          , 
            In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
	language = {en},
	number = {15},
	urldate = {2023-12-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	month = apr,
	year = {2021},
	pages = {e2016239118},
	file = {Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:/Users/lenatrnovec/Zotero/storage/2G7E47DF/Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:application/pdf},
}

@article{wang_pre-trained_2024,
	title = {Pre-trained {Language} {Models} in {Biomedical} {Domain}: {A} {Systematic} {Survey}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Pre-trained {Language} {Models} in {Biomedical} {Domain}},
	url = {https://dl.acm.org/doi/10.1145/3611651},
	doi = {10.1145/3611651},
	abstract = {Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing tasks. This also benefits the biomedical domain: researchers from informatics, medicine, and computer science communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It is nontrivial to make a survey that not only systematically reviews recent advances in biomedical PLMs and their applications but also standardizes terminology and benchmarks. This article summarizes the recent progress of pre-trained language models in the biomedical domain and their applications in downstream biomedical tasks. Particularly, we discuss the motivations of PLMs in the biomedical domain and introduce the key concepts of pre-trained language models. We then propose a taxonomy of existing biomedical PLMs that categorizes them from various perspectives systematically. Plus, their applications in biomedical downstream tasks are exhaustively discussed, respectively. Last, we illustrate various limitations and future trends, which aims to provide inspiration for the future research.},
	language = {en},
	number = {3},
	urldate = {2023-12-01},
	journal = {ACM Computing Surveys},
	author = {Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Chen, Zhihong and Tiwari, Prayag and Li, Zhao and Fu, Jie},
	month = mar,
	year = {2024},
	pages = {1--52},
	file = {Wang et al. - 2024 - Pre-trained Language Models in Biomedical Domain .pdf:/Users/lenatrnovec/Zotero/storage/4HIHSADX/Wang et al. - 2024 - Pre-trained Language Models in Biomedical Domain .pdf:application/pdf},
}

@article{katoh-kurasawa_transcriptional_2021,
	title = {Transcriptional milestones in \textit{{Dictyostelium}} development},
	volume = {31},
	issn = {1088-9051, 1549-5469},
	url = {http://genome.cshlp.org/lookup/doi/10.1101/gr.275496.121},
	doi = {10.1101/gr.275496.121},
	abstract = {Dictyostelium
              development begins with single-cell starvation and ends with multicellular fruiting bodies. Developmental morphogenesis is accompanied by sweeping transcriptional changes, encompassing nearly half of the 13,000 genes in the genome. We performed time-series RNA-sequencing analyses of the wild type and 20 mutants to explore the relationships between transcription and morphogenesis. These strains show developmental arrest at different stages, accelerated development, or atypical morphologies. Considering eight major morphological transitions, we identified 1371 milestone genes whose expression changes sharply between consecutive transitions. We also identified 1099 genes as members of 21 regulons, which are groups of genes that remain coordinately regulated despite the genetic, temporal, and developmental perturbations. The gene annotations in these groups validate known transitions and reveal new developmental events. For example, DNA replication genes are tightly coregulated with cell division genes, so they are expressed in mid-development although chromosomal DNA is not replicated. Our data set includes 486 transcriptional profiles that can help identify new relationships between transcription and development and improve gene annotations. We show its utility by showing that cycles of aggregation and disaggregation in allorecognition-defective mutants involve dedifferentiation. We also show sensitivity to genetic and developmental conditions in two commonly used actin genes,
              act6
              and
              act15
              , and robustness of the
              coaA
              gene. Finally, we propose that
              gpdA
              is a better mRNA quantitation standard because it is less sensitive to external conditions than commonly used standards. The data set is available for democratized exploration through the web application dictyExpress and the data mining environment Orange.},
	language = {en},
	number = {8},
	urldate = {2023-12-01},
	journal = {Genome Research},
	author = {Katoh-Kurasawa, Mariko and Hrovatin, Karin and Hirose, Shigenori and Webb, Amanda and Ho, Hsing-I and Zupan, Blaž and Shaulsky, Gad},
	month = aug,
	year = {2021},
	pages = {1498--1511},
	file = {Katoh-Kurasawa et al. - 2021 - Transcriptional milestones in Dictyostelium.pdf:/Users/lenatrnovec/Zotero/storage/CRXPZC2J/Katoh-Kurasawa et al. - 2021 - Transcriptional milestones in Dictyostelium.pdf:application/pdf},
}

@article{luo_improving_2022,
	title = {Improving language model of human genome for {DNA}–protein binding prediction based on task-specific pre-training},
	issn = {1913-2751, 1867-1462},
	url = {https://link.springer.com/10.1007/s12539-022-00537-9},
	doi = {10.1007/s12539-022-00537-9},
	abstract = {The DNA–protein binding plays a pivotal role in regulating gene expression and evolution, and computational identification of DNA–protein has drawn more and more attention in bioinformatics. Recently, variants of BERT are also used to capture the semantic information of DNA sequences for predicting DNA–protein bindings. In this study, we leverage a task-specific pre-training strategy on BERT using large-scale multi-source DNA–protein binding data and present TFBert. TFBert treats DNA sequences as natural sentences and k-mer nucleotides as words. It can effectively extract upstream and downstream nucleotide context information by pre-training the 690 unlabeled ChIP-seq datasets. Experiments show that the pre-trained model can achieve promising performance on every single dataset in the 690 ChIP-seq datasets after simple fine tuning, especially on small datasets. The average AUC is 94.7\%, outperforming existing popular methods. In conclusion, this study provides a variant of BERT based on pre-training and achieved state-of-the-art results in predicting DNA–protein bindings. We believe that TFBert can provide insights into other biological sequence classification problems.},
	language = {en},
	urldate = {2023-12-01},
	journal = {Interdisciplinary Sciences: Computational Life Sciences},
	author = {Luo, Hanyu and Shan, Wenyu and Chen, Cheng and Ding, Pingjian and Luo, Lingyun},
	month = sep,
	year = {2022},
	file = {Luo et al. - 2022 - Improving language model of human genome for DNA–p.pdf:/Users/lenatrnovec/Zotero/storage/92I2SLEC/Luo et al. - 2022 - Improving language model of human genome for DNA–p.pdf:application/pdf},
}

@misc{ghosh_predicting_2023,
	title = {Predicting {Transcription} {Factor} {Binding} {Sites} using {Transformer} based {Capsule} {Network}},
	url = {http://arxiv.org/abs/2310.15202},
	abstract = {Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERTCap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A549, GM12878, Hep-G2, H1-hESC and Hela, available in the ENCODE repository. The results show that the average area under the receiver operating characteristic curve score exceeds 0.91 for all such five cell lines. DNABERT-Cap is also compared with existing state-of-the-art deep learning based predictors viz. DeepARC, DeepTF, CNN-Zeng and DeepBind, and is seen to outperform them.},
	language = {en},
	urldate = {2023-12-01},
	publisher = {arXiv},
	author = {Ghosh, Nimisha and Santoni, Daniele and Saha, Indrajit and Felici, Giovanni},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15202 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Genomics},
	file = {Ghosh et al. - 2023 - Predicting Transcription Factor Binding Sites usin.pdf:/Users/lenatrnovec/Zotero/storage/G9ZIB9R8/Ghosh et al. - 2023 - Predicting Transcription Factor Binding Sites usin.pdf:application/pdf},
}

@article{avsec_effective_2021,
	title = {Effective gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01252-x},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {Abstract
            
              How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequences through the use of a deep learning architecture, called Enformer, that is able to integrate information from long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Furthermore, Enformer learned to predict enhancer–promoter interactions directly from the DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of human disease associations and provide a framework to interpret
              cis
              -regulatory evolution.},
	language = {en},
	number = {10},
	urldate = {2023-12-01},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R. and Grabska-Barwinska, Agnieszka and Taylor, Kyle R. and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	pages = {1196--1203},
	file = {Avsec et al. - 2021 - Effective gene expression prediction from sequence.pdf:/Users/lenatrnovec/Zotero/storage/8FW2ZDPD/Avsec et al. - 2021 - Effective gene expression prediction from sequence.pdf:application/pdf},
}
